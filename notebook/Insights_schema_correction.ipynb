{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Insights Module Ingestion - Schema Correction\n",
        "\n",
        "This notebook demonstrates the utility of the OEA_py class notebook, while correcting module tables initially ingested without headers and incorrect data types.\n",
        "\n",
        "Tables are read from ```stage2/Ingested/M365/v1.14``` and written out, with the corrected schema, to ```stage2/Ingested_Corrected/M365/v1.14```\n",
        "\n",
        "The steps outlined below describe how this notebook is used to correct the Microsoft Education Insights module tables:\n",
        "- Set the workspace for where the table schemas are to be corrected. \n",
        "- 4 functions are defined and used:\n",
        "   1. **_extract_element**: uses the Insights metadata to extract the correct column names.\n",
        "   2. **_dtype_config**: uses the Insights metadata to extract the correct column dtypes.\n",
        "   3. **correct_insights_table_schema**: uses the corrected column names and dtypes to correct the schema per table given to the function.\n",
        "   4. **correct_insights_dataset**: extracts the names of all the folders currently stored in stage2/Ingested/M365, corrects the schema per table using the function above, and overwrites the tables with the updated schemas.\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "workspace = 'dev'\n",
        "version = '1.4.0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%run OEA_py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# 1) set the workspace (this determines where in the data lake you'll be writing to and reading from).\n",
        "# You can work in 'dev', 'prod', or a sandbox with any name you choose.\n",
        "# For example, Sam the developer can create a 'sam' workspace and expect to find his datasets in the data lake under oea/sandboxes/sam\n",
        "oea.set_workspace(workspace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# 2) schema correction, since Insights data initially landed doesn't have column headers\n",
        "\n",
        "def _extract_element(lst, element_num=0):\n",
        "    return [item[element_num] for item in lst]\n",
        "\n",
        "def _dtype_config(dtype_lst):\n",
        "    return [item.capitalize() + 'Type()' for item in dtype_lst]\n",
        "\n",
        "def correct_insights_table_schema(df, table_name):\n",
        "    list_of_column_names = _extract_element(metadata[table_name])\n",
        "    list_of_column_dtypes = _extract_element(metadata[table_name], 1)\n",
        "    list_of_column_dtypes = _dtype_config(list_of_column_dtypes)\n",
        "\n",
        "    n = 0\n",
        "    df_updatedColumns = df\n",
        "    for c in df.columns:\n",
        "        if c != 'rundate':\n",
        "            new_col_name = list_of_column_names[n]\n",
        "            df_updatedColumns = df_updatedColumns.withColumnRenamed(c, new_col_name)\n",
        "            if list_of_column_dtypes[n] != 'StringType()':\n",
        "                if list_of_column_dtypes[n] == 'IntegerType()':\n",
        "                    df_updatedColumns = df_updatedColumns.withColumn(new_col_name, df_updatedColumns[new_col_name].cast(IntegerType()))\n",
        "                elif list_of_column_dtypes[n] == 'TimestampType()':\n",
        "                    df_updatedColumns = df_updatedColumns.withColumn(new_col_name, df_updatedColumns[new_col_name].cast(TimestampType()))\n",
        "                elif list_of_column_dtypes == 'ShortType()':\n",
        "                    df_updatedColumns = df_updatedColumns.withColumn(new_col_name, df_updatedColumns[new_col_name].cast(ShortType()))\n",
        "                elif list_of_column_dtypes[n] == 'LongType()':\n",
        "                    df_updatedColumns = df_updatedColumns.withColumn(new_col_name, df_updatedColumns[new_col_name].cast(LongType()))\n",
        "                elif list_of_column_dtypes[n] == 'DoubleType()':\n",
        "                    df_updatedColumns = df_updatedColumns.withColumn(new_col_name, df_updatedColumns[new_col_name].cast(DoubleType()))\n",
        "                elif list_of_column_dtypes[n] == 'DateType()':\n",
        "                    df_updatedColumns = df_updatedColumns.withColumn(new_col_name, df_updatedColumns[new_col_name].cast(DateType()))\n",
        "                elif list_of_column_dtypes[n] == 'BooleanType()':\n",
        "                    df_updatedColumns = df_updatedColumns.withColumn(new_col_name, df_updatedColumns[new_col_name].cast(BooleanType()))\n",
        "        else:\n",
        "            df_updatedColumns = df_updatedColumns\n",
        "        n = n + 1\n",
        "    return df_updatedColumns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def correct_insights_dataset(tables_source, write_destination):\n",
        "    items = oea.get_folders(tables_source)\n",
        "    for item in items: \n",
        "        if item == 'metadata.csv':\n",
        "            logger.info('ignore metadata processing, since this is not a table to be ingested')\n",
        "        else:\n",
        "            table_path = tables_source +'/'+ item\n",
        "            mode = 'append' if item == 'activity' else 'overwrite'\n",
        "            df = spark.read.format('delta').load(oea.to_url(table_path))\n",
        "            df_corrected = correct_insights_table_schema(df, table_name=item)\n",
        "            df_corrected.write.format('delta').mode(mode).save(oea.to_url(write_destination + '/' +item))\n",
        "            logger.info('Successfully corrected the schema for table: ' + item + ' from: ' + table_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "metadata = oea.get_metadata_from_url('https://raw.githubusercontent.com/aanseacorelearn/education_insight/main/test_data/metadata.csv')\n",
        "correct_insights_dataset('stage2/Ingested/M365/v' + version, 'stage2/Ingested_Corrected/M365/v' + version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df = spark.read.format('delta').load(oea.to_url('stage2/Ingested_Corrected/M365/v'+ version + '/activity'), header='true')\n",
        "display(df.limit(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
